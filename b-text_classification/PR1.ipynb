{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR1: Binary classification with Logistic Regression.\n",
    "\n",
    "In this notebook we are going to learn how to classify image patches as text and non-text objects.\n",
    "\n",
    "First we are going to see an introduction to Logistic Regression using synthetic data. Then, we will go into a real world problem for text detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Logistic Regression\n",
    "\n",
    "We are going train a binary classifier using the Logistic Regression model: $h_\\theta(\\textbf{x}) = \\frac{1}{1+e^{-\\theta^\\intercal \\textbf{x}}}$\n",
    "\n",
    "For this we need to fit the parameters $\\theta$ to our dataset by minimizing the cost function $J$:\n",
    "\n",
    "$\\hat{\\theta} = \\underset{\\theta}{\\text{minimize}} {1 \\over m} \\sum_{i=1}^m{-y^{(i)}\\log(h_\\theta(x^{(i)}) - (1-y^{(i)})\\log(1-h_\\theta(x^{(i)})}$\n",
    "\n",
    "where $(x^{(i)},y^{(i)})$ is the i-th training sample, and $m$ is the number of samples in the training set.\n",
    "\n",
    "We are going to use the Gradient Descent algorithm to solve this minimization problem, in pseudo-code:\n",
    "\n",
    "repeat until convergence (or max. number of iterations) {\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ (for all j)\n",
    "\n",
    "}\n",
    "\n",
    "where $\\alpha$ is the learning rate, and the partial derivative (the gradient) of the cost function is given by:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = {1 \\over m} \\sum_{i=1}^m{(h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}}$\n",
    "\n",
    "Notice that the partial derivative of our cost function has the same form as for the Linear Regression cost function, however the difference is in the term $h_\\theta(\\textbf{x})$, our model. In the case of Logistic Regression $h_\\theta(\\textbf{x})$ is the sigmoid function of $\\theta^\\intercal \\textbf{x}$.\n",
    "\n",
    "Thus, the Python code of our **GradientDescent** function is going to be quite the same as for Linear Regression, but calling the **sigmoid** function to evaluate the Logistic Regression model $h_\\theta(\\textbf{x})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(X):\n",
    "    '''\n",
    "    Computes the Sigmoid function of the input argument X.\n",
    "    '''\n",
    "    return 1.0/(1+np.exp(-X))\n",
    "\n",
    "\n",
    "def GradientDescent(x,y,max_iterations=2500, alpha=0.1):\n",
    "    \n",
    "    m,n = x.shape # number of samples, number of features\n",
    "\n",
    "    # y must be a column vector\n",
    "    y = y.reshape(m,1)\n",
    "    \n",
    "    #initialize the parameters\n",
    "    theta = np.ones(shape=(n,1)) \n",
    "    \n",
    "    # Repeat until convergence (or max_iterations)\n",
    "    for iteration in range(max_iterations):\n",
    "        h = sigmoid(np.dot(x,theta))\n",
    "        error = (h-y)\n",
    "        gradient = np.dot(x.T , error) / m\n",
    "        theta = theta - alpha*gradient\n",
    "    return theta\n",
    "\n",
    "\n",
    "def classifyVector(X, theta):\n",
    "    '''\n",
    "    Evaluate the Logistic Regression model h(x) with theta parameters,\n",
    "    and returns the predicted label of x.\n",
    "    '''\n",
    "    prob = sigmoid(sum(np.dot(X,theta)))\n",
    "    if prob > 0.5: return 1.0\n",
    "    else: return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use this code in a synthetic dataset. First load the data and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the synthetic dataset\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('./PR1data1.pkl','rb') as f:\n",
    " (X,y) = pickle.load(f)\n",
    "\n",
    "print X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], marker='o', c='b') #positive samples\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], marker='x', c='r') #negative samples\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the Logistic Regression classifier and draw its decision boundary in a 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Append the x_0 column (for the bias term theta_0)\n",
    "x = np.ones(shape=(X.shape[0], 1))\n",
    "x = np.append(x,X,axis=1)\n",
    "\n",
    "#Logistic Regression gradient descent optimization\n",
    "w = GradientDescent(x,y)\n",
    "\n",
    "#Evaluate the classifier accuracy in the training data\n",
    "H = [classifyVector(x[i,:],w) for i in range(x.shape[0])]\n",
    "print \"Training Accuracy : \"+str(float(np.sum(H == y)) / y.shape[0])\n",
    "\n",
    "#Plot data\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], marker='o', c='b') #positive samples\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], marker='x', c='r') #negative samples\n",
    "\n",
    "#Plot Decision Boundary\n",
    "u = np.linspace(-1, 1.5, 50)\n",
    "v = np.linspace(-1, 1.5, 50)\n",
    "z = np.zeros(shape=(len(u), len(v)))\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        z[i, j] = sigmoid(np.dot(np.array([1,u[i],v[j]]),w))\n",
    "\n",
    "z = z.T\n",
    "\n",
    "cs = plt.contour(u, v, z, levels=[0.5])\n",
    "plt.clabel(cs, inline=1, fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the previous plot the classification boundary with 2 features fits very poorly our data.\n",
    "\n",
    "One way to fit the data better is to create more features from each data point. We will map the features  into all polynomial terms of $x_1$ and $x_2$ up to the sixth power. \n",
    "\n",
    "$MapFeature(x_1,x_2) = [x_1,x_2,x_1^2,x_1x_2,x_2^2,x_1^3, \\dots , x_1x_2^5, x_2^6]$\n",
    "\n",
    "As a result of this mapping, our vector of two features is transformed into a 28-D vector. The Logistic Regression classifier trained on this high dimensional feature space will have a more complex decision boundary (i.e. more degrees of freedom) and will appear nonlinear when drawn in the 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_feature(x1, x2):\n",
    "    '''\n",
    "    Maps 2D features to quadratic features.\n",
    "    Returns a new feature vector with more features, comprising of\n",
    "    x1, x2, x1^2, x2^2, x1*x2, x1*x2^2, etc...\n",
    "    '''\n",
    "    x1.shape = (x1.size, 1)\n",
    "    x2.shape = (x2.size, 1)\n",
    "    degree = 6\n",
    "    out = np.ones(shape=(x1[:, 0].size, 1))\n",
    "    \n",
    "    for i in range(1, degree + 1):\n",
    "        for j in range(i + 1):\n",
    "            r = (x1 ** (i - j)) * (x2 ** j)\n",
    "            out = np.append(out, r, axis=1)\n",
    "\n",
    "    return out\n",
    "\n",
    "features = map_feature(X[:,0],X[:,1])\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression gradient descent optimization\n",
    "w = GradientDescent(features,y)\n",
    "\n",
    "H = [classifyVector(features[i,:],w) for i in range(features.shape[0])]\n",
    "print \"Training Accuracy : \"+str(float(np.sum(H == y)) / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Plot data\n",
    "plt.scatter(X[y==1, 0], X[y==1, 1], marker='o', c='b') #positive samples\n",
    "plt.scatter(X[y==0, 0], X[y==0, 1], marker='x', c='r') #negative samples\n",
    "\n",
    "#Plot Boundary\n",
    "u = np.linspace(np.min(X[:,0]), np.max(X[:,0]), 50)\n",
    "v = np.linspace(np.min(X[:,1]), np.max(X[:,1]), 50)\n",
    "z = np.zeros(shape=(len(u), len(v)))\n",
    "for i in range(len(u)):\n",
    "    for j in range(len(v)):\n",
    "        z[i, j] = sigmoid(map_feature(np.array(u[i]),np.array(v[j])).dot(w))\n",
    "        \n",
    "z = z.T\n",
    "cs = plt.contour(u, v, z, levels=[0.5])\n",
    "plt.clabel(cs, inline=1, fontsize=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 1: Gradient Descent algorithm convergence</font>\n",
    "\n",
    "The code of our **GradientDescent** function loops over the max. number of iterations (2500 by default). This does not guarantee that the algorithm has converged to the global optimum of the cost function.\n",
    "\n",
    "Modify the code of the **GradientDescent** function to loop until convergence **OR** to reach the max. iterations. Try to execute the function for different **max_iterations** and **alpha** values and see if you can make it converge.\n",
    "\n",
    "What is the best Training Accuracy you can reach? \n",
    "\n",
    "Did you find the global minima? How many iterations does it need? With which $\\alpha$ value? \n",
    "\n",
    "Plot the decision boundary of your classifier.\n",
    "\n",
    "Hints:\n",
    "\n",
    "* Reaching the global minima may take a very large number of iterations. Take it easy and do not get stuck here, just give it a try. The important goal of this exercise is to get an intuition of what the gradient descent algorithm is doing, and why it may take so long to reach the global optima.\n",
    "* Remember that the $\\alpha$ parameter controls the step size of Gradient Descent. Try different values. Note that the value of the step size $\\alpha$ is allowed to change at every iteration.\n",
    "* The Numpy function **np.array_equal(A,B)** returns True if arrays A and B are equal.\n",
    "* To determine whether Gradient Descent has converged, it may help to print out the value of $J(\\theta)$ during each iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 2: Regularized Logistic Regression</font>\n",
    "\n",
    "Whether you have found the global minima in the previous exercise, or a very good training accuracy, it is probable that you are overfitting the classifier to this particular training data. If this is the case, the classifier is not going to perform well on unseen data examples.\n",
    "\n",
    "Regularization is a technique to prevent overfitting.\n",
    "\n",
    "In order to implement Regularized Logistic Regression we must change only the term of the partial derivative (the gradient) of the cost function:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = {1 \\over m} \\sum_{i=1}^m{(h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}}$.\n",
    "\n",
    "which in the case of Regularized Logistic Regression is given by:\n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = {1 \\over m} \\sum_{i=1}^m{(h_\\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}+{\\lambda \\over m}\\theta_j}$\n",
    "\n",
    "Implement the Regularized Logistic Regression in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR1: Our first binary classifier for Text vs. Non-text classification.\n",
    "\n",
    "Our goal is to classify image patches as text and non-text objects.\n",
    "\n",
    "The roadmap is as follows:\n",
    "\n",
    "* Learn how to load images as NumPy arrays with SciPy.\n",
    "* Import the train/test datasets.\n",
    "* Train and Run the Logistic Regression classifier using two different image features:\n",
    " * Raw pixels.\n",
    " * Histograms of grey values.\n",
    "* Evaluate and compare the classifier results using different evaluation measures. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "\n",
    "Load images, convert to numpy arrays, reshape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import misc #micellanea submodule (for imread)\n",
    "\n",
    "#Load an image example\n",
    "img = misc.imread('./img_A.pgm')\n",
    "\n",
    "type(img),img.ndim, img.shape, img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt #matplotlib plotting library\n",
    "\n",
    "# Display an image\n",
    "plt.imshow(img, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reshape image array to a vector (1D array)\n",
    "img = np.reshape(img,-1)\n",
    "\n",
    "type(img),img.ndim, img.shape, img.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition (Optional)\n",
    "\n",
    "This part of the notebook deals with loading images from your hard disk and convert them to NumPy arrays. This is going to be our dataset. Images are stored as pgm files but may be in any other image format. \n",
    "\n",
    "Since we are going to read near 12000 files from disk, it may take around 20 minutes. If you prefer you can jump to the next section of the notebook and load directly the **raw_pixels_dataset_5980.pklz** file, which already contains the NumPy arrays for the entire dataset. The code is here just in case you are curious about how to convert image files into NumPy arrays, and how the **raw_pixels_dataset_5980.pklz** has been created.\n",
    "\n",
    "To execute this part of the notebook you must download the **scene_text_dataset.zip** (<font color='red'>~140 Mb</font>) file from the Campus Virtual site.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import listdir #now we will load all images in a given directory\n",
    "\n",
    "datapath = 'data/characters/icdar/img_ICDAR_train/'\n",
    "\n",
    "char_raw_pixels = np.array([ np.reshape(misc.imread(datapath+f),-1) for f in listdir(datapath) ])\n",
    "char_raw_pixels = np.reshape(char_raw_pixels,[-1,1024])\n",
    "\n",
    "char_raw_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "datapath = 'data/background/train/'\n",
    "\n",
    "bg_raw_pixels = np.array([ np.reshape(misc.imread(datapath+f),-1) for f in listdir(datapath) ])\n",
    "bg_raw_pixels = np.reshape(bg_raw_pixels,[-1,1024])\n",
    "\n",
    "bg_raw_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We want a balanced dataset so we take only the first 5980 background samples\n",
    "bg_raw_pixels = bg_raw_pixels[0:5980,:]\n",
    "bg_raw_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize. Just to be sure the data is correct\n",
    "\n",
    "im = char_raw_pixels[1,:]\n",
    "im = np.reshape(im,[32,32])\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(im, cmap=plt.cm.gray)\n",
    "\n",
    "im = bg_raw_pixels[1,:]\n",
    "im = np.reshape(im,[32,32])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(im, cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_features = np.append(char_raw_pixels,bg_raw_pixels, axis=0)\n",
    "\n",
    "char_labels = np.ones([char_raw_pixels.shape[0],1])\n",
    "bg_labels   = np.zeros([bg_raw_pixels.shape[0],1])\n",
    "\n",
    "train_labels = np.append(char_labels, bg_labels)\n",
    "\n",
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we now do the same for test data\n",
    "\n",
    "datapath = 'data/characters/icdar/img_ICDAR_test/'\n",
    "\n",
    "char_raw_pixels = np.array([ np.reshape(misc.imread(datapath+f),-1) for f in listdir(datapath) ])\n",
    "char_raw_pixels = np.reshape(char_raw_pixels,[-1,1024])\n",
    "\n",
    "char_raw_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = 'data/background/test/'\n",
    "\n",
    "bg_raw_pixels = np.array([ np.reshape(misc.imread(datapath+f),-1) for f in listdir(datapath) ])\n",
    "bg_raw_pixels = np.reshape(bg_raw_pixels,[-1,1024])\n",
    "\n",
    "bg_raw_pixels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_features = np.append(char_raw_pixels,bg_raw_pixels, axis=0)\n",
    "\n",
    "char_labels = np.ones([char_raw_pixels.shape[0],1])\n",
    "bg_labels   = np.zeros([bg_raw_pixels.shape[0],1])\n",
    "\n",
    "test_labels = np.append(char_labels, bg_labels)\n",
    "\n",
    "test_features.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now we can save all our data as python serialized data, so we do not need to read again image\n",
    "# files the next time we want execute our classification code\n",
    "\n",
    "import pickle #module for serialization of python object structure\n",
    "import gzip   #we can compress our data directly when writting data to a file\n",
    "\n",
    "with gzip.open('./raw_pixels_dataset_5980.pklz','wb') as f:\n",
    " pickle.dump((train_labels,train_features,test_labels,test_features),f,pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification using raw pixels as features\n",
    "\n",
    "In the following we will try how good are the raw pixel features to automatically classify the different classes.\n",
    "\n",
    "We are going to evaluate classification using Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('./raw_pixels_dataset_5980.pklz','rb') as f:\n",
    " (train_labels,train_features,test_labels,test_features) = pickle.load(f)\n",
    "\n",
    "print train_features.shape\n",
    "print test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression gradient descent optimization \n",
    "w1 = GradientDescent(train_features,train_labels,500) # try only 500 iterations (this is going to take a long time)\n",
    "\n",
    "# Calculate classification Accuracy in train data\n",
    "H = [classifyVector(train_features[i,:],w1) for i in range(train_features.shape[0])]\n",
    "print \"Train Accuracy : \"+str(float(np.sum(H == train_labels)) / train_labels.shape[0])\n",
    "\n",
    "# Note: you may want to save the learned parameters here if you do not want \n",
    "# to re-train each time you load the notebook!\n",
    "\n",
    "#import pickle #module for serialization of python object structure\n",
    "#import gzip   #we can compress our data directly when writting data to a file\n",
    "#with gzip.open('./raw_pixels_learned_parameters.pklz','wb') as f:\n",
    "# pickle.dump((w1,f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate classification Accuracy in test data\n",
    "H = [classifyVector(test_features[i,:],w1) for i in range(test_features.shape[0])]\n",
    "print \"Test Accuracy : \"+str(float(np.sum(H == test_labels)) / test_labels.shape[0])\n",
    "\n",
    "# Plot the confusion matrix on test data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_labels, H)\n",
    "print 'Confusion matrix:'\n",
    "print (cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "We have seen how using the raw pixels is not a good idea. Intuitively there are two main reasons for the bad performance of our classifier: first, we do not have enough data to train in such a high dimensional space (1024-D), second, the raw pixels do not have enough discriminative power to effectively discriminate over the Text and Non-text examples in our dataset. Notice that a simple 1-pixel shift in one of the examples may produce a very different feature vector.\n",
    "\n",
    "In Computer Vision (and in Pattern Recognition in general), feature extraction is a procedure to extract pieces of information which are relevant for solving the computational task at hand.\n",
    "There is a large tradition in designing handcrafted features, that incorporate class prior knowledge, to solve specific problems.\n",
    "\n",
    "In this part of the notebook we are going to extract simple features: histograms of the intensity values of our images. The intuition is that in text image patches we expect to find bi-level histograms (two opposite dominant colors), because text is by design written with high contrast to its background.\n",
    "\n",
    "Then we will evaluate how good those features are for  automatically classifying between the two classes (Text/Non-text) using Logistic Regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For each example we compute the histogram of grey intensity values\n",
    "\n",
    "new_train_features = np.zeros([train_features.shape[0],8])\n",
    "for i in range(train_features.shape[0]):\n",
    "    new_train_features[i,:] = np.histogram(train_features[i,:],8)[0]\n",
    "    new_train_features[i,:] /= np.sum(new_train_features[i,:]) #Histogram normalization\n",
    "    \n",
    "new_test_features = np.zeros([test_features.shape[0],8])\n",
    "for i in range(test_features.shape[0]):\n",
    "    new_test_features[i,:] = np.histogram(test_features[i,:],8)[0]\n",
    "    new_test_features[i,:] /= np.sum(new_test_features[i,:]) #Histogram normalization\n",
    "    \n",
    "new_train_features.shape, new_test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Visualize the histograms of positive/negative samples\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(np.reshape(train_features[1,:],[32,32]), cmap=plt.cm.gray)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(np.reshape(train_features[5981,:],[32,32]), cmap=plt.cm.gray)\n",
    "\n",
    "bins = [0,1,2,3,4,5,6,7]\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(bins, new_train_features[1,:], align='center')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(bins, new_train_features[5981,:], align='center')\n",
    "\n",
    "#print train_labels[1],train_labels[5981]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Logistic Regression gradient descent optimization\n",
    "w1 = GradientDescent(new_train_features,train_labels)\n",
    "\n",
    "H = [classifyVector(new_train_features[i,:],w1) for i in range(new_train_features.shape[0])]\n",
    "print \"Train Accuracy : \"+str(float(np.sum(H == train_labels)) / train_labels.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate classification Accuracy in test data\n",
    "H = [classifyVector(new_test_features[i,:],w1) for i in range(new_test_features.shape[0])]\n",
    "print \"Test Accuracy : \"+str(float(np.sum(H == test_labels)) / test_labels.shape[0])\n",
    "\n",
    "\n",
    "# confusion matrix on test data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(test_labels, H)\n",
    "print 'Confusion matrix:'\n",
    "print (cm)\n",
    "\n",
    "# Plot confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the use of Histograms of intensity values we have improved the performance of our classifier by more than 20%!\n",
    "\n",
    "Notice that this is a very simple feature extraction process, that is not really used in this way in state-of-the-art algorithms. In fact the proposed features are quite weak (as can be seen with the obtained results). However, the idea here is to take conscience that the design of handcrafted features is a possible way of improving the discrimination power of our classifiers.\n",
    "\n",
    "This experiment also serves to introduce the topic of next Practical (PR2), where we are going to see how it is possible to automatically learn powerful features in an unsupervised way from our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 3: Precision/Recall Curve</font>\n",
    "\n",
    "Making use the Histogram of intensity values features, evaluate the Precision and Recall measures at different operation points of the classifier. This can be done by changing the 0.5 decision threshold in the ClassifyVector function to a range of values between 0 and 1. Plot the obtained Precision/Recall curve and analyse it.\n",
    "\n",
    "By looking at the curve: What is the (approximate) Precision to be expected if we want to tune our classifier to a Recall value of 0.9?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 4: Open Exercise (optional)</font>\n",
    "\n",
    "Propose and implement an improvement, or an extra evaluation analysis, for our Text vs. Non-text classifier in its current status.\n",
    "\n",
    "Some ideas (not a closed list):\n",
    "\n",
    "* Implement the Stochastic Gradient Descent algorithm. Show how it improves the training time performance.\n",
    "\n",
    "* Use cross-validation to tune the meta-parameters (max_iterations and learning rate $\\alpha$) of Gradient Descent. \n",
    "\n",
    "* Other optimization algorithms?\n",
    "\n",
    "* Evaluate the Test Accuracy as a function of the number of training examples ($m$).\n",
    "\n",
    "* Use Histogram of Oriented Gradients (HOG) image features. http://scikit-image.org/docs/dev/auto_examples/plot_hog.html\n",
    "\n",
    "* Improve the Histogram features. E.g. Evaluate the effect of histogram size. Try different number of bins and compare the obatined results (show precision, recall, accuracy). Can you improve the current Test Accuracy? What is the best accuracy you can reach?\n",
    "\n",
    "* Zoning: compute NxN grey level histograms in a N by N grid of cells over the image patch, and concatenate them to create a new feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annex: Sliding Window with our Text vs. Non-text classifier\n",
    "\n",
    "Sliding Window is a common Computer Vision technique used to apply patch-based classifiers into full-size images. The basic idea is to exhaustively evaluate the classifier response in \"all\" possible sub-windows of the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import misc \n",
    "#pylab.rcParams['figure.figsize'] = 14, 10.5  # changes the default image size for the notebook\n",
    "plt.figure(num=None, figsize=(14, 10.5), dpi=80, facecolor='w', edgecolor='k')\n",
    "\n",
    "# Load an image and plot it\n",
    "img = misc.imread('img_scene.jpg')\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img, cmap=plt.cm.gray)\n",
    "\n",
    "detection_map = np.zeros(shape=img.shape)\n",
    "\n",
    "win_sizes = (32,64,96)\n",
    "win_step  = 0.2\n",
    "\n",
    "for size in win_sizes:\n",
    "  for x in range(0,img.shape[1]-size,int(size*win_step)):\n",
    "    for y in range(0,img.shape[0]-size,int(size*win_step)):\n",
    "        window = img[y:y+size,x:x+size]\n",
    "        window = misc.imresize(window,(32,32))\n",
    "        raw_pixels = np.reshape(window,[-1,1024])\n",
    "        hist_feature = np.histogram(raw_pixels,8)[0]\n",
    "        hist_feature = hist_feature.astype(np.float32)\n",
    "        hist_feature /= np.sum(hist_feature.T)\n",
    "        prob = sigmoid(sum(np.dot(hist_feature,w1)))\n",
    "        detection_map[y:y+size,x:x+size] += 255*prob\n",
    "        \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(detection_map,cmap=plt.cm.jet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
